{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import nltk #import library nltk\n",
    "from nltk.tokenize import word_tokenize #import word_tokenize for tokenizing text into words \n",
    "from nltk.tokenize import sent_tokenize #import sent_tokenize for tokenizing paragraph into sentences\n",
    "from nltk.stem.porter import PorterStemmer #import Porter Stemmer Algorithm \n",
    "from nltk.stem import WordNetLemmatizer #import WordNet lemmatizer \n",
    "from nltk.corpus import stopwords #import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory #import Indonesian Stemmer\n",
    "import re #import regular expression\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('artikel1.txt', 'r')\n",
    "t = file.read()\n",
    "text_data = t\n",
    "\n",
    "#casefolding\n",
    "def casefolding(s):\n",
    "    new_str = s.lower()  \n",
    "    return new_str\n",
    "\n",
    "cf = casefolding(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation from string\n",
    "def removepunct(str):\n",
    "    new_string =  re.sub(r\"[\\W]\", \" \", str)\n",
    "    return new_string\n",
    "\n",
    "rp = removepunct(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove digit from string\n",
    "def removeDigit(str):\n",
    "    new_string =  re.sub(r\"[0-9]\", \" \", str)\n",
    "    return new_string\n",
    "\n",
    "rd = removeDigit(rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove words in length 1-3\n",
    "def removelg(str):\n",
    "    new_string =  re.sub(' \\w{1,3} ', ' ', str)\n",
    "    return new_string\n",
    "    \n",
    "rl = removelg(rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove multiple space\n",
    "def removespace(str):\n",
    "    new_string = re.sub(' +', ' ',str)\n",
    "    return new_string\n",
    "\n",
    "rms = removespace(rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming Indonesian\n",
    "def stemmingIndo(str):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    return stemmer.stem(str)\n",
    "\n",
    "stindo = stemmingIndo(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "def stpwrds(str):\n",
    "    stop_words = set(stopwords.words('indonesian'))\n",
    "    word_tokens = word_tokenize(stindo) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]  \n",
    "    filtered_sentence = [] \n",
    "    \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "\n",
    "filt = stpwrds(stindo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = ' '.join(filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenization(s):\n",
    "    tokens = word_tokenize(s)\n",
    "    return tokens\n",
    "wordtoken = word_tokenization(par)\n",
    "bowD = wordtoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordSet = set(bowD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDict = dict.fromkeys(wordSet,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in bowD:\n",
    "    wordDict[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(bowCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = computeTF(wordDict, bowD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10((1+N) / float(val))\n",
    "        \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = computeIDF([wordDict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = computeTFIDF(tf, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  tfidf\n",
      "halal          0.035787\n",
      "indonesia      0.014736\n",
      "industri       0.010526\n",
      "pasar          0.008420\n",
      "posisi         0.006315\n",
      "lifestyle      0.006315\n",
      "negara         0.006315\n",
      "center         0.006315\n",
      "global         0.006315\n",
      "produk         0.006315\n",
      "produsen       0.006315\n",
      "sektor         0.006315\n",
      "tingkat        0.006315\n",
      "forum          0.004210\n",
      "milik          0.004210\n",
      "makan          0.004210\n",
      "konferensi     0.004210\n",
      "anwar          0.004210\n",
      "selenggara     0.004210\n",
      "uang           0.004210\n",
      "alih           0.004210\n",
      "utama          0.004210\n",
      "ekosistem      0.004210\n",
      "atur           0.004210\n",
      "cipta          0.004210\n",
      "dunia          0.004210\n",
      "buah           0.002105\n",
      "conference     0.002105\n",
      "nirwandar      0.002105\n",
      "oktober        0.002105\n",
      "...                 ...\n",
      "masuk          0.002105\n",
      "convention     0.002105\n",
      "jakarta        0.002105\n",
      "gaya           0.002105\n",
      "efektif        0.002105\n",
      "economic       0.002105\n",
      "hidup          0.002105\n",
      "hotel          0.002105\n",
      "hubung         0.002105\n",
      "indikator      0.002105\n",
      "dukung         0.002105\n",
      "internasional  0.002105\n",
      "international  0.002105\n",
      "islam          0.002105\n",
      "islamic        0.002105\n",
      "kait           0.002105\n",
      "malaysia       0.002105\n",
      "kenal          0.002105\n",
      "kerja          0.002105\n",
      "domestik       0.002105\n",
      "direktur       0.002105\n",
      "konsumen       0.002105\n",
      "koordinasi     0.002105\n",
      "kuat           0.002105\n",
      "laku           0.002105\n",
      "data           0.002105\n",
      "main           0.002105\n",
      "dasar          0.002105\n",
      "maksud         0.002105\n",
      "ketua          0.002105\n",
      "\n",
      "[83 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'tfidf': tfidf})\n",
    "#test = df.sort_values('tfidf', ascending=False)\n",
    "test = df.sort_values(by = 'tfidf', ascending=False)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
